<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<title>research - Goodluck Oguzie</title>
		<link rel="stylesheet" href="dist/reset.css" />
		<link rel="stylesheet" href="dist/reveal.css" />
		<link rel="stylesheet" href="dist/theme/white.css" />
		<link rel="stylesheet" href="plugin/highlight/monokai.css" />

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>

		<link rel="stylesheet" href="mycss.css" />

		<script type="text/javascript">
			window.addEventListener("load", function() {
			  var revealDiv = document.querySelector("body div.reveal");
			  var footer = document.querySelector(".footer");
			  revealDiv.appendChild(footer);
			});
		</script>

		<style>
			.reveal .slide-number {
				font-size: 24pt;
				color: #000000;
			}
			video::-webkit-media-controls-panel {
         		background-image: linear-gradient(transparent, transparent) !important;
    		}
		</style>

	</head>
	<body>
		
		<div class="reveal">
			<div class="slides">
				<!-- =========== TITLE SLIDE =========== -->
				<section data-background="img/aston_title_background.jpg" style="color: white;">
					<div style="text-align: left; margin: 0 auto; width: 100%; font-size: 1em;">
						  Enhancing Robot Social Navigation with<br>
						  Reinforcement Learning and Advanced <br>
						        Predictive Models
					</div>
					<div style="text-align: left; margin: 0 auto; width: 100%; font-size: 0.9em;">
						  Goodluck Oguzie
					</div>
					<div style="text-align: left; margin: 0 auto; width: 100%; font-size: 0.9em;">
						  190212683@aston.ac.uk
					</div>
				</section>

				<!-- =========== CONTENT SLIDE (UPDATED, CENTERED, WHITE TEXT, NUMBERED, FIXED BACKGROUND) =========== -->
				<section data-background="img/aston_slides_background_22.png" data-background-size="cover" data-background-position="top" data-transition="slide">
					<h2 style="color: white; text-align: center;">Content</h2>
					<div style="text-align: center; font-size: 0.85em; color: white; padding-top: 100px;">
						<ol style="list-style-position: inside;">
							<li class="fragment">Context and scope:<br><span style="font-size: 0.75em;">Brief introduction to my field of study, goals and objectives of the PhD</span></li>
							<li class="fragment">Reinforcement Learning (RL):<br><span style="font-size: 0.75em;">Introduction to RL and how it enables robots to learn optimal navigation strategies through trial and error</span></li>
							<li class="fragment">Predictive World Models for Social Navigation:<br><span style="font-size: 0.75em;">Use of predictive models to predict future states and improve decision-making in dynamic environments</span></li>
							<li class="fragment">Cosine-Gated LSTM (CGLSTM) for Prediction:<br><span style="font-size: 0.75em;">Development of CGLSTM to improve sequence prediction by integrating cosine-based gating mechanisms</span></li>
							<li class="fragment">Adaptive Prediction Horizons:<br><span style="font-size: 0.75em;">Introduction of an entropy-driven mechanism to dynamically adjust prediction horizons based on environmental uncertainty</span></li>
							<li class="fragment">Conclusion:<br><span style="font-size: 0.75em;">Conclusion of the work done during this 4 years</span></li>
						</ol>
					</div>
					<aside class="notes">
						Briefly outline the structure of your presentation: context, RL, predictive models, technical innovations, adaptive methods, and concluding remarks.
					</aside>
				</section>

				<!-- =========== CONTEXT AND SCOPE: SLIDE 1 - WHAT IS SOCIAL ROBOT NAVIGATION? (UPDATED BACKGROUND) =========== -->
				<section data-background="img/aston_slides_background_22.png" data-background-size="cover" data-background-position="0 -20px" data-transition="slide">
					<h2 style="color: white; text-align: center;">What is Social Robot Navigation?</h2>
					<div style="text-align: center; font-size: 0.85em; color: white; padding-top: 100px;">
						<ul>
							<li class="fragment">Definition: Robots navigating safely in human-populated environments, respecting social norms.</li>
							<li class="fragment">Importance: Critical for seamless integration into daily life, ensuring safety and social acceptance.</li>
							<li class="fragment">Applications: Healthcare robots, hospitality robots, public space navigation.</li>
						</ul>
						<div class="fragment" style="padding-top: 20px;">
							<video muted data-autoplay src="img/socialrobot.mp4" width="70%" controls="controls"></video>
						</div>
					</div>
					<aside class="notes">
						Explain what social robot navigation means, its importance for real-world applications, and how the video illustrates robots interacting with humans.
					</aside>
				</section>

				<!-- =========== OUTLINE SLIDE (REFINED) =========== -->
				<section data-background="#f5f5f5" data-transition="fade">
					<h2 style="color: #333333; text-align: center;">Outline</h2>
					<ul style="font-size: 0.9em; list-style-type: square; text-align: center;">
						<li class="fragment">A bit about me: My background and journey in robotics</li>
						<li class="fragment">Lines of research: Key areas driving my work</li>
						<li class="fragment">Social robot navigation: Enhancing autonomy with RL and predictive models</li>
						<li class="fragment">Hint at the future: Future directions in robotics research</li>
					</ul>
					<aside class="notes">
						Introduce the presentation structure: personal background, research focus, main topic, and future outlook. Keep it brief to transition smoothly.
					</aside>
				</section>

				<!-- =========== ABOUT ME SLIDE =========== -->
				<section data-background="img/aston_background.jpg" data-transition="slide">
					<h3>About Me</h3>
					<div style="text-align: left; font-size: 0.85em;">
						<ul>
							<li class="fragment">Goodluck Oguzie</li>
							<li class="fragment">PhD in Robotics, <a href="https://robolab.unex.es/">Universidad de Extremadura</a>, Cáceres, Spain
								<div class="fragment" style="text-align: center;">
									<img src="img/caceres.jpg" width="70%" style="border: 2px solid white;"/>
								</div>
							</li>
							<li class="fragment">PhD Focus: Robotics, Active Perception, POMDP & Particle Filters</li>
							<li class="fragment">Career: Joined Aston University in August 2018 as Lecturer, now Senior Lecturer</li>
							<li class="fragment">Lab: <a href="https://arp-lab.com">Autonomous Robotics and Perception Lab</a></li>
						</ul>
					</div>
					<aside class="notes">
						Highlight your journey from PhD to current role, and briefly mention how it ties to social navigation research.
					</aside>
				</section>

				<!-- =========== LINES OF RESEARCH SLIDE =========== -->
				<section>
					<h2>Lines of research</h2>
					<ul>
						<li>human pose estimation <b>&</b> monitoring</li>
						<li>monocular depth estimation <b>&</b> traffic understanding</li>
						<li>social navigation</li>
					</ul>
				</section>

				<!-- =========== HUMAN TRACKING AND MONITORING SLIDE =========== -->
				<section>
					<section>
						<h4>Human tracking and monitoring</h4>
						<ul style="font-size: 0.8em; line-height: 1.1;">
							<li>Motivation: experience from previous projects <b>&</b> the need for tracking in Social Robot Navigation</li>
							<li class="fragment">Applicable in real-life conditions</li>
							<li class="fragment">Multi-camera, multi-person, full-skeleton, self-supervised</li>
							<li class="fragment">Project with Legrand Care, £244,512 (January 2024, 30 months).</li>
							<li class="fragment">With PhD students D. Rodriguez-Criado (former), V. Gbouna and PDRA Maria Vicini.</li>
						</ul>
						<span class="fragment">
							<video muted data-autoplay id="video_hpe" src="resources/sevilla2.mp4" width="540px" controls="controls"></video>
							<video data-autoplay id="video_hpe" src="resources/hpe.mp4" width="630px" controls="controls"></video>
						</span>
						<br/>
						<div class="reveal" style="width: 70%; margin-left: auto; margin-right: auto; line-height: 1.06;">
							<span style="font-family: monospace; font-size: 0.45em; text-align: left; display: inline-block;">
								<b style="color: red;">[1]</b> Rodriguez-Criado D, Bachiller-Burgos P, Vogiatzis G, Manso LJ. Multi-person 3D pose estimation from unlabelled data.
								<u>Machine Vision and Applications, Springer</u>. 2024 May;35(3):1-8.
							</span>
						</div>
					</section>

					<section>
						<embed src="pdfs/hpe.pdf" width="90%" height="900px"/>
					</section>
				</section>

				<!-- Continue with remaining sections unchanged... -->

			</div>
		</div>

		<script>
			Reveal.initialize({
				hash: true,
				width: 1600,
				height: 900,
				slideNumber: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>

		<!-- <div class="footer">
			<span style="font-size: 0.7em;">https://ljmanso.github.io/r</span>
		</div> -->
	</body>
</html>
